{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_models\n",
    "\n",
    "Train the face detection and landmark localisation models for Zeno's face based on our annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import dlib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    from ConfigParser import ConfigParser    # If using Python 2.7\n",
    "except ImportError:\n",
    "    from configparser import ConfigParser    # If using Python 3.5\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the data structure for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468 face detection samples and 130 landmark localisation have been prepared.\n",
      "955 face detection samples and 275 landmark localisation have been prepared.\n",
      "All 1200 face detection samples and 349 landmark localisation have been prepared.\n"
     ]
    }
   ],
   "source": [
    "# Load images and annotation\n",
    "annotations = pd.read_pickle(os.path.realpath(os.path.join('./dataset', 'annotations.pkl')))\n",
    "face_detection_scale = config.getfloat('facial_landmark_tracker', 'face_detection_scale')\n",
    "face_detection_images = []\n",
    "face_detection_groundtruth = []\n",
    "shape_predictor_images = []\n",
    "shape_predictor_groundtruth = []\n",
    "last_check_time = time.time()\n",
    "for idx in range(annotations.shape[0]):\n",
    "    entry = annotations.iloc[idx]\n",
    "    image_path = os.path.realpath(os.path.join('./dataset', entry['session'], \n",
    "                                               '%06d.png' % entry['index']))\n",
    "    if type(entry['face_box']) != type(np.nan) or not np.isnan(entry['face_box']):\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY)\n",
    "        image_size = (image.shape[1], image.shape[0])\n",
    "        face_detection_size = (max(round(image_size[0] * face_detection_scale), 1), \n",
    "                               max(round(image_size[1] * face_detection_scale), 1))\n",
    "        if face_detection_size != image_size:\n",
    "            face_detection_images.append(cv2.resize(image, face_detection_size))\n",
    "        else:\n",
    "            face_detection_images.append(face_detection_image)\n",
    "        face_box = [int(round(x * face_detection_scale)) for x in list(entry['face_box'])]\n",
    "        face_detection_groundtruth.append([dlib.rectangle(face_box[0], face_box[1], \n",
    "                                                          face_box[0] + face_box[2] - 1, \n",
    "                                                          face_box[1] + face_box[3] - 1)])\n",
    "        if type(entry['landmarks']) != type(np.nan) or not np.isnan(entry['landmarks']):\n",
    "            shape_predictor_images.append(image)\n",
    "            bounding_box = dlib.rectangle(entry['face_box'][0], entry['face_box'][1], \n",
    "                                          entry['face_box'][0] + entry['face_box'][2] - 1, \n",
    "                                          entry['face_box'][1] + entry['face_box'][3] - 1)\n",
    "            landmarks = [dlib.point(int(round(pts[0])), int(round(pts[1]))) \n",
    "                         for pts in entry['landmarks']]\n",
    "            shape_predictor_groundtruth.append([dlib.full_object_detection(bounding_box, \n",
    "                                                                           landmarks)])\n",
    "    current_time = time.time()\n",
    "    if last_check_time < current_time - 10.0:\n",
    "        last_check_time = current_time\n",
    "        print('%d face detection samples and %d landmark localisation have been prepared.' % \n",
    "              (len(face_detection_groundtruth), len(shape_predictor_groundtruth)))\n",
    "print('All %d face detection samples and %d landmark localisation have been prepared.' % \n",
    "      (len(face_detection_groundtruth), len(shape_predictor_groundtruth)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the face detector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the shape predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape predictor model has been trained and saved to: D:\\hhj\\zeno_face_tracker\\models\\zeno_face_tracker_model.dat\n"
     ]
    }
   ],
   "source": [
    "options = dlib.shape_predictor_training_options()\n",
    "options.be_verbose = config.getboolean('shape_predictor_training_options', \n",
    "                                       'be_verbose')\n",
    "options.cascade_depth = config.getint('shape_predictor_training_options', \n",
    "                                      'cascade_depth')\n",
    "options.feature_pool_region_padding = config.getfloat('shape_predictor_training_options', \n",
    "                                                      'feature_pool_region_padding')\n",
    "options.feature_pool_size = config.getint('shape_predictor_training_options', \n",
    "                                          'feature_pool_size')\n",
    "options.lambda_param = config.getfloat('shape_predictor_training_options', \n",
    "                                       'lambda_param')\n",
    "options.nu = config.getfloat('shape_predictor_training_options', 'nu')\n",
    "options.num_test_splits = config.getint('shape_predictor_training_options', \n",
    "                                        'num_test_splits')\n",
    "options.num_trees_per_cascade_level = config.getint('shape_predictor_training_options', \n",
    "                                                    'num_trees_per_cascade_level')\n",
    "options.oversampling_amount = config.getint('shape_predictor_training_options', \n",
    "                                            'oversampling_amount')\n",
    "options.tree_depth = config.getint('shape_predictor_training_options', 'tree_depth')\n",
    "shape_predictor = dlib.train_shape_predictor(images, groundtruth, options)\n",
    "shape_predictor_model_path = os.path.realpath(os.path.join('./models', 'zeno_face_tracker_model.dat'))\n",
    "shape_predictor.save(shape_predictor_model_path)\n",
    "print('Shape predictor model has been trained and saved to: ' + shape_predictor_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the model on live video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lala.dat')n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_pickle(os.path.realpath(os.path.join('./dataset', 'annotations.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(annotations.iloc[0]['aligned_landmarks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_scale = max(config.getfloat('facial_landmark_tracker', \n",
    "                                           'face_detection_scale'), 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detection_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
