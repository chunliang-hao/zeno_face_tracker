{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare_dataset\n",
    "\n",
    "Track Zeno's face images using an [exisiting tool](https://github.com/IntelligentBehaviourUnderstandingGroup/dlib_and_chehra_stuff) and prepare them for manual annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "import shutil\n",
    "import numpy as np\n",
    "try:\n",
    "    from ConfigParser import ConfigParser    # If using Python 2.7\n",
    "except ImportError:\n",
    "    from configparser import ConfigParser    # If using Python 3.5\n",
    "config = ConfigParser()\n",
    "config.read('prepare_dataset.ini')\n",
    "sys.path.append(os.path.realpath(config.get('facial_landmark_tracker', 'repository_path')))\n",
    "import ibug_face_tracker\n",
    "from zeno_face_tracker_helpers import *\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialise the landmark localiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facial landmark localiser initialised.\n"
     ]
    }
   ],
   "source": [
    "tracker = ibug_face_tracker.FaceTracker(os.path.realpath(config.get('facial_landmark_tracker', \n",
    "                                                                    'ert_model_path')), \n",
    "                                        os.path.realpath(config.get('facial_landmark_tracker', \n",
    "                                                                    'auxiliary_model_path')))\n",
    "tracker.face_detection_scale = config.getfloat('facial_landmark_tracker', 'face_detection_scale')\n",
    "tracker.minimum_face_size = config.getint('facial_landmark_tracker', 'minimum_face_size')\n",
    "tracker.hard_failure_threshold = -1e6\n",
    "tracker.estimate_head_pose = False\n",
    "tracker.eye_iterations = 0\n",
    "print('Facial landmark localiser initialised.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Localise Facial Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 images to be processed.\n",
      "112 images have been processed.\n",
      "222 images have been processed.\n",
      "341 images have been processed.\n",
      "462 images have been processed.\n",
      "579 images have been processed.\n",
      "682 images have been processed.\n",
      "795 images have been processed.\n",
      "905 images have been processed.\n",
      "1012 images have been processed.\n",
      "1123 images have been processed.\n",
      "Done, all 1200 images have been processed, 1034 of which have landmarks localised.\n"
     ]
    }
   ],
   "source": [
    "# Enumerate jobs\n",
    "source_images = sorted(glob.glob(os.path.realpath(os.path.join('./dataset', 'cam*', '*.png'))))\n",
    "source_images = [x for x in source_images if '.pts.png' not in x]\n",
    "print('%d images to be processed.' % len(source_images))\n",
    "\n",
    "# Get landmarks\n",
    "successes = 0\n",
    "last_check_time = time.time()\n",
    "for idx, image_path in enumerate(source_images):\n",
    "    image = cv2.imread(image_path)\n",
    "    tracker.reset()\n",
    "    tracker.track(image)\n",
    "    if tracker.has_facial_landmarks:\n",
    "        pts_path = os.path.splitext(image_path)[0] + '.init.pts'\n",
    "        save_pts(pts_path, tracker.facial_landmarks)\n",
    "        tracker.plot_current_result(image)\n",
    "        rendering_path = pts_path + '.png'\n",
    "        cv2.imwrite(rendering_path, image)\n",
    "        successes += 1\n",
    "    current_time = time.time()\n",
    "    if current_time - last_check_time > 10.0:\n",
    "        last_check_time = current_time\n",
    "        print('%d images have been processed.' % (idx + 1))\n",
    "print('Done, all %d images have been processed, %d of which have landmarks localised.' % \n",
    "      (len(source_images), successes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare the annotation jobs than can be loaded in FLAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_000.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_000.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_001.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_001.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_002.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_002.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_003.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_003.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_004.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_004.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_005.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_005.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_006.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_006.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_007.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_007.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_008.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_008.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_009.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_009.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_010.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_010.fad.bak\n",
      "Annotation job created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_011.fad\n",
      "Backup file created: D:\\hhj\\zeno_face_tracker\\dataset\\batch_011.fad.bak\n"
     ]
    }
   ],
   "source": [
    "# Enumerate the recording sessions\n",
    "recording_sessions = sorted(glob.glob(os.path.realpath(os.path.join('./dataset', 'cam*'))))\n",
    "recording_sessions = [x for x in recording_sessions if os.path.isdir(x)]\n",
    "\n",
    "# How many examples per session?\n",
    "num_samples = 1e6\n",
    "for session in recording_sessions:\n",
    "    images = glob.glob(os.path.join(session, '*.png'))\n",
    "    images = [x for x in images if '.pts.' not in x]\n",
    "    num_samples = min(num_samples, len(images))\n",
    "\n",
    "# Permutate samples in the sessions so that each annotation batch gets a bit of everything\n",
    "indices = np.zeros((len(recording_sessions), num_samples), dtype=int)\n",
    "for idx in range(indices.shape[1]):\n",
    "    indices[:, idx] = np.random.permutation(indices.shape[0])\n",
    "\n",
    "# Create jobs\n",
    "for batch_idx, batch in enumerate(indices):\n",
    "    batch_content = []\n",
    "    for idx, session in enumerate(batch):\n",
    "        image_path = os.path.join(recording_sessions[session], '%06d.png' % idx)\n",
    "        pts_path = os.path.splitext(image_path)[0] + '.init.pts'\n",
    "        if os.path.exists(pts_path):\n",
    "            pts = load_pts(pts_path)\n",
    "            if pts.shape[0] == 68:\n",
    "                \n",
    "                batch_content.append((image_path, pts))\n",
    "    job_path = os.path.realpath(os.path.join('./dataset', 'batch_%03d.fad' % batch_idx))\n",
    "    duplication = 0\n",
    "    while True:\n",
    "        if os.path.exists(job_path):\n",
    "            duplication += 1\n",
    "            job_path = os.path.realpath(os.path.join(\n",
    "                './dataset', 'batch_%03d.%03d.fad' % (batch_idx, duplication)))\n",
    "        else:\n",
    "            break\n",
    "    save_annotation_job(job_path, batch_content, 68)\n",
    "    print('Annotation job created: ' + job_path)\n",
    "    backup_path = job_path + '.bak'\n",
    "    shutil.copyfile(job_path, backup_path)\n",
    "    print('Backup file created: ' + backup_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
