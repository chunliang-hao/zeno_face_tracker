{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_landmark_localiser\n",
    "\n",
    "Train the landmark localiser for Zeno using dlib's implementation of the ensemble of regression trees (ERT) methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import dlib\n",
    "import time\n",
    "import pandas as pd\n",
    "try:\n",
    "    from ConfigParser import ConfigParser    # If using Python 2.7\n",
    "except ImportError:\n",
    "    from configparser import ConfigParser    # If using Python 3.5\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "print('All modules imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the data structure for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 349 samples have been prepared.\n"
     ]
    }
   ],
   "source": [
    "# Load images and annotation\n",
    "annotations = pd.read_pickle(os.path.realpath(os.path.join('./dataset', 'annotations.pkl')))\n",
    "images = []\n",
    "groundtruth = []\n",
    "last_check_time = time.time()\n",
    "for idx in range(annotations.shape[0]):\n",
    "    entry = annotations.iloc[idx]\n",
    "    image_path = os.path.realpath(os.path.join('./dataset', entry['session'], \n",
    "                                               '%06d.png' % entry['index']))\n",
    "    images.append(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2GRAY))\n",
    "    bounding_box = dlib.rectangle(entry['face_box'][0], entry['face_box'][1], \n",
    "                                  entry['face_box'][0] + entry['face_box'][2] - 1, \n",
    "                                  entry['face_box'][1] + entry['face_box'][3] - 1)\n",
    "    landmarks = [dlib.point(int(round(pts[0])), int(round(pts[1]))) \n",
    "                 for pts in entry['landmarks']]\n",
    "    groundtruth.append([dlib.full_object_detection(bounding_box, landmarks)])\n",
    "    current_time = time.time()\n",
    "    if last_check_time < current_time - 10.0:\n",
    "        last_check_time = current_time\n",
    "        print('%d samples have been prepared.' % (idx + 1))\n",
    "print('All %d samples have been prepared.' % annotations.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the shape predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape predictor model has been trained and saved to: D:\\hhj\\zeno_face_tracker\\models\\zeno_face_tracker_model.dat\n"
     ]
    }
   ],
   "source": [
    "options = dlib.shape_predictor_training_options()\n",
    "options.be_verbose = config.getboolean('shape_predictor_training_options', \n",
    "                                       'be_verbose')\n",
    "options.cascade_depth = config.getint('shape_predictor_training_options', \n",
    "                                      'cascade_depth')\n",
    "options.feature_pool_region_padding = config.getfloat('shape_predictor_training_options', \n",
    "                                                      'feature_pool_region_padding')\n",
    "options.feature_pool_size = config.getint('shape_predictor_training_options', \n",
    "                                          'feature_pool_size')\n",
    "options.lambda_param = config.getfloat('shape_predictor_training_options', \n",
    "                                       'lambda_param')\n",
    "options.nu = config.getfloat('shape_predictor_training_options', 'nu')\n",
    "options.num_test_splits = config.getint('shape_predictor_training_options', \n",
    "                                        'num_test_splits')\n",
    "options.num_trees_per_cascade_level = config.getint('shape_predictor_training_options', \n",
    "                                                    'num_trees_per_cascade_level')\n",
    "options.oversampling_amount = config.getint('shape_predictor_training_options', \n",
    "                                            'oversampling_amount')\n",
    "options.tree_depth = config.getint('shape_predictor_training_options', 'tree_depth')\n",
    "shape_predictor = dlib.train_shape_predictor(images, groundtruth, options)\n",
    "shape_predictor_model_path = os.path.realpath(os.path.join('./models', 'zeno_face_tracker_model.dat'))\n",
    "shape_predictor.save(shape_predictor_model_path)\n",
    "print('Shape predictor model has been trained and saved to: ' + shape_predictor_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the model on live video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lala.dat')n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
